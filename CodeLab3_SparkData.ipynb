{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Технологии обработки больших данных\n",
    "\n",
    "Занятие 3. PySpark Data Structures\n",
    "\n",
    "0. Запуск PySpark на локальной машине\n",
    "1. Spark DataFrame \n",
    "2. Spark RDD (разбор предыдущего ДЗ)\n",
    "3. Spark Pandas API DataFrame\n",
    "4. Домашнее задание "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq8U3BtmhtRx"
   },
   "source": [
    "\n",
    "## 0. Запуск PySpark на локальной машине\n",
    "\n",
    "При первом запуске spark на локальной машине нужно установить следующие зависимости:  \n",
    "\n",
    "- Java 8 \n",
    "- Apache Spark 3.2.1 with hadoop 3.2, \n",
    "- Findspark to locate the spark in the system. \n",
    "\n",
    "Если работаете на локальной машине и зависимости уже установлены, эту ячейку запускать не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lh5NCoc8fsSO"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "tar xvf spark-3.2.1-bin-hadoop3.2.tgz > /dev/null\n",
    "pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зависимости для работы Pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install pandas pyarrow plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начните с этой ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1b8k_OVf2QF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = os.getcwd() + \"/spark-3.2.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_Uz1NL4gHFx"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример данных [German Credit](https://www.kaggle.com/uciml/german-credit), которые используются для решении задачи кредитного скоринга. Это небольшой датасет с информацией о клиентах, необходимой для принятия решения - выдавать кредит или нет.  \n",
    "\n",
    "Сегодня мы не будем решать задачу предсказания, просто разберемся с основными приемами EDA (Exploratory data analysis, [Разведочный анализ данных](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D0%B2%D0%B5%D0%B4%D0%BE%D1%87%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'sample_data/credit_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columns**  \n",
    "\n",
    "Age (numeric)  \n",
    "Sex (text: male, female)  \n",
    "Job (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)  \n",
    "Housing (text: own, rent, or free)  \n",
    "Saving accounts (text - little, moderate, quite rich, rich)  \n",
    "Checking account (numeric, in DM - Deutsch Mark)  \n",
    "Credit amount (numeric, in DM)  \n",
    "Duration (numeric, in month)  \n",
    "Purpose (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS3cVqWkC7gJ"
   },
   "source": [
    "## 1. Spark DataFrame\n",
    "\n",
    "Базовый класс для работы со структуированными данными в pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlMUL-zkBs0N",
    "outputId": "4d8c8886-7d8c-47ae-bfa9-0380f5e78a5b"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(DATA_PATH, header=True)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_nvZ0qNDF9-",
    "outputId": "b70543f1-a2d6-4a4f-a74f-1c0f1a0aeaab",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First rows in this DataFrame\n",
    "df.show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Схема данных как в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"id INT, Age INT, Sex STRING, Job INT, Housing STRING, Saving_accounts STRING, \\\n",
    "Checking_account STRING, Credit_amount INT, Duration INT, Purpose STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('sample_data/credit_data.csv', schema=schema, header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сортировка и фильтрация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One column sorting\n",
    "df.sort('Job', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Few columns sorting\n",
    "df.sort(['Age', 'Credit_amount'], ascending=[False, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_car = df.filter(df[\"Purpose\"] == 'car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доля автокредитов\n",
    "df_car.count() / df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Группировка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Age\").count().sort('Age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark RDD \n",
    "\n",
    "Resilient Distributed Dataset. \n",
    "Менее удобный, но более производительный контейнер для данных.  \n",
    "\n",
    "Подробнее про DataFrame, DataSet и RDD на русском языке\n",
    "[1](https://www.bigdataschool.ru/blog/spark-sql-data-structures.html), \n",
    "[2](https://www.bigdataschool.ru/blog/rdd-vs-dataframe-vs-dataset.html).  \n",
    "\n",
    "На английском рекомендую [официальный гайд](https://spark.apache.org/docs/latest/sql-getting-started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = spark.read.text('log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Note, we cant use   lambda x:   x.value.upper()\n",
    "df = log_file.rdd.map(lambda x: ( x.value.upper() ,) ).toDF() \n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('white_list.txt') as f:\n",
    "    ww = f.readlines()\n",
    "\n",
    "ww = \"\".join([w for w in ww]).split()\n",
    "ww = list(map(str.lower, ww))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_filter(s):\n",
    "    w = s.value.split()\n",
    "    timestamp = str(w[0]) + str(w[1]) + \" \"\n",
    "    words = w[2:]\n",
    "    filtered_words = list(filter(lambda x: x in ww, words))\n",
    "    return (timestamp, \" \".join([w for w in filtered_words])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df2 = log_file.rdd.map(white_filter).toDF(schema=('TimeStamp', 'Words'))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = df2.rdd.flatMap(lambda line: line[1].split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wordCounts.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark Pandas API \n",
    "\n",
    "Начиная с версии Spark 3.2 имеется реализация Pandas API.    \n",
    "Хороший материал непосредственно по pandas: [mlcourse.ai](https://habr.com/ru/company/ods/blog/322626/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "import pyspark.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data/credit_data.csv')\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Средний возраст заемщиков \n",
    "df['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Статистика по всем числовым колонкам\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Индексация и фильтрация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Индексация python slices \n",
    "df[1:11:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фильтрация по условию\n",
    "df[df[\"Sex\"] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какой средний размер кредита у заемщиков мужчин?\n",
    "df[df[\"Sex\"] == 'male']['Credit_amount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Какой средний размер кредита у заемщиков женщин?\n",
    "df[df[\"Sex\"] == 'female']['Credit_amount'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Группировка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка разделяет df на несколько частей, в которых значения заданной колонки будут одинаковыми\n",
    "df.groupby('Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно указать какие колонки нас интересуют \n",
    "df.groupby('Sex')['Credit_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В конце группировки нужно указать функцию\n",
    "df.groupby('Sex')['Credit_amount'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Домашнее задание  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Сколько мужчин и женщин (признак Sex) представлено в этом наборе данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Каков средний возраст (признак Age) женщин?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Какова доля заемщиков с собственным жильем (признак Housing)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Каково среднее значение возраста тех, кто имеет высокие накопления (признак Saving_accounts)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Каково среднеквадратичное отклонения возраста тех, кто имеет высокие накопления (признак Saving_accounts)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Выведите гистограмму категорий покупок (признак Purpose) для мужчин и женщин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. На что чаще всего берутся длинные кредиты (более 24 мес)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Какой средний срок кредита (признак Duration) для заемщиков, имеющих высокие текущие траты (признак Checking_account)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Какой средний срок кредита (признак Duration) для заемщиков, имеющих низкие текущие траты (признак Checking_account)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. На какую цель взят самый дорогой кредит?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PySparkColab_1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
